---
title: "Final Project: Depression Prediction Using Machine Learning Techniques"
output: pdf_document
---

# Group members: Write your group members here.
Kai Yeh


# Load packages
```{r}
library(glmnet)
library(data.table)
library(ggplot2)
library(knitr)
library(ROCR)
library(gridExtra)

```

# Prepare
```{r}
rm(list = ls())
set.seed(132435)

```

#### 1. Load the data and summarize.

To summarize the data you should provide summary statistics such mean, standard deviation, max, min, etc. for each variable in data.train.
```{r}
data.full <- read.csv("train.csv") 
data.full <- data.full[sample(nrow(data.full)),-c(1,3)] # drop irrelevant variables for analysis, randomize data
missing_prop <- sapply(data.full, 
                       function(x){mean(is.na(x))}) 
data.full <- data.full[missing_prop <= 0] # drop varaiables with missing data

data.train <- data.full[1:floor(nrow(data.full)*0.8),] # training set

data.test <- data.full[ceiling(nrow(data.full)*0.8):nrow(data.full), ] # test set

# Your code to summarize data.train below.
summary(data.train)
```

#### 2. Fit logistic regression model

Complete the code below. Hint: check R's glm function.
```{r}
fit <- glm(depressed ~., family=binomial(link='logit'), data=data.train, ) # fit your logistic model to the training set here.
```

#### 3. Evaluate the model fit


```{r}
compute.metrics = function(y.predict, p.predict, y.test) {
  # Parameters
  # y.predict : predictions generated by some model.
  # p.predict : predicted probabilities generated by some model.
  # y.test : outcomes in test data set.
  
  N.correct <- sum(y.predict==y.test,na.rm=TRUE)
  accuracy <- N.correct/length(y.test)
  brier.score <- (1/length(p.predict)) * sum((y.test-p.predict)^2)
  
  true.positive <- sum(y.predict == y.test & y.predict > 0, na.rm=TRUE)
  false.positive <- sum(y.predict != y.test & y.predict > 0, na.rm=TRUE)
  true.negative <- sum(y.predict == y.test & y.predict == 0, na.rm=TRUE)
  false.negative <- sum(y.predict != y.test & y.predict == 0, na.rm=TRUE)
  
  
  precision <- true.positive/(true.positive + false.positive)
  recall <- true.positive/(true.positive+false.negative)
  specificity <- true.negative/(false.positive+true.negative)
  
  f1 <- 2*(1/recall + 1/precision)^(-1) # F1 score
  
  return(data.table(accuracy=round(accuracy, digits = 3), brier.score=round(brier.score, digits = 3),
                    true.positive=true.positive,
                    false.positive=false.positive,
                    true.negative=true.negative,
                    false.negative=false.negative,
         precision= round(precision, digits = 3), recall= round(recall, digits = 3),
         specificity= round(specificity, digits = 3), f1= round(f1, digits = 3)))
}


evaluate.model <- function(p.predict, theta, y.test) {
  # Parameters
  # p.predict : predicted probabilities generated by some model.
  # theta: decision threshold
  # y.test : outcomes in test data set

  y.predict <- ifelse(p.predict>=theta, 1, 0) # predictions given decision threshold theta
  
  out <- compute.metrics(y.predict, p.predict, y.test)
  
  return(out)
}


sweep.theta = function(thetas, p.predict, y.test) {
  # Parameters
  # thetas : range of thetas to sweep
  # p.predict : predicted probabilities generated by some model.
  # y.test : outcomes in test data set

  evals = list()
  for (i in 1:length(thetas)) {
      c.eval <- evaluate.model(p.predict,thetas[i],y.test) # evaluate model at theta[i]
      evals <- c(evals, list(c.eval))
  }
  evals.table = rbindlist(evals)
  evals.table$theta = thetas
  evals.table.m = melt(evals.table[, -c('true.positive', 
                                        'true.negative',
                                        'false.positive',
                                      'false.negative'), with=F], id.var='theta')
  
  return(list(evals.table = evals.table,
              evals.table.m=evals.table.m))
  
}
```

Plot (accuracy, brier.score, precision, recall, specificity, f1) vs theta in single plot.
```{r}
Y.test <- data.test[, c("depressed")]
X.test <- data.test[,-which(names(data.test) == "depressed")]

thetas <- seq(0,0.6, by=0.01)
p.predict.logistic <- predict(fit, newdata=data.test, type='response') # predicted probabilities generated by fit (logistic regression model)
out <- sweep.theta(thetas, p.predict.logistic, Y.test)
to.plot.logistic <- out$evals.table.m
metrics.logistic <- out$evals.table

# Your plot here. Hint: use ggplot.

ggplot(metrics.logistic, aes(theta, accuracy, colour = "accuracy")) + 
  geom_line() +
  geom_line(aes(theta, brier.score, colour = "brier")) +
  geom_line(aes(theta, precision, colour = "precision")) +
  geom_line(aes(theta, recall, colour = "recall")) +
  geom_line(aes(theta, specificity, colour = "specificity")) +
  geom_line(aes(theta, f1, colour = "f1")) +
  labs(colour = "Metrics") +
  labs(y = "Ratio") 
 

```

Find the in- and out-of sample confusion matrix for $\theta=0.15$. 
```{r}
max.acc.theta <- 0.15

Y.train <- data.train[, "depressed"]
p.predict.logistic.train <- predict(fit,newdata=data.train,type='response') # in-sample predicted probabilities generated by fit (logistic regression model)
confusion.matrix.train <- evaluate.model(p.predict.logistic.train, max.acc.theta, Y.train)

confusion.matrix.test <- evaluate.model(p.predict.logistic, max.acc.theta, Y.test)
confusion.matrix <- rbind(confusion.matrix.train, confusion.matrix.test)
rownames(confusion.matrix) <- c("train","test")
confusion.matrix


```

Next, use k-fold cross-validation to test whether the model is overfitting

```{r}

## k-fold cross validation
## goal: determine if we are overfitting our model with logistic regression 

k <- 100 
max.acc.theta <- 0.15 ## selected to achieve the highest recall while getting accaptable results from other metrices

max.rec.test <- NULL
max.pre.test <- NULL
max.rec.train <- NULL
max.pre.train <- NULL


for(i in 1:k){

data.full <- read.csv("train.csv") 
data.full <- data.full[sample(nrow(data.full)),-c(1,3)] # drop irrelevant variables for analysis, randomize data
missing_prop <- sapply(data.full, 
                       function(x){mean(is.na(x))}) 
data.full <- data.full[missing_prop <= 0] # drop varaiables with missing data

data.train <- data.full[1:floor(nrow(data.full)*0.95),] # training set

data.test <- data.full[ceiling(nrow(data.full)*0.95):nrow(data.full), ] # test set

fit <- glm(depressed ~., family=binomial(link='logit'), data=data.train, ) # fit your logistic model to the training set here.

Y.test <- data.test[, c("depressed")]
p.predict.logistic <- predict(fit, newdata=data.test, type='response') # out-sample predicted probabilities generated by fit (logistic regression model)
max.rec.test[i] <- evaluate.model(p.predict.logistic, max.acc.theta, Y.test)$recall
max.pre.test[i] <- evaluate.model(p.predict.logistic, max.acc.theta, Y.test)$precision

Y.train <- data.train[, "depressed"]
p.predict.logistic.train <- predict(fit,newdata=data.train,type='response') # in-sample predicted probabilities generated by fit (logistic regression model)
max.rec.train[i] <- evaluate.model(p.predict.logistic.train, max.acc.theta, Y.train)$recall
max.pre.train[i] <- evaluate.model(p.predict.logistic.train, max.acc.theta, Y.train)$precision

}

output <- matrix(data=NA, ncol=2, nrow =2)
rownames(output) <- c("train","test")
colnames(output) <- c("Recall", "Precision")

output[1,] <- c(mean(max.rec.train), mean(max.pre.train))
output[2,] <- c(mean(max.rec.test), mean(max.pre.test))
output

df <- data.frame(t(rbind(max.rec.train, max.rec.test, max.pre.train, max.pre.test)))

# histogram
p1 <- ggplot(df, aes(x=max.rec.train)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_vline(aes(xintercept=mean(max.rec.train)), color="blue", linetype="dashed", size=1) + geom_text(aes(x=mean(max.rec.train)+0.05, label=round(mean(max.rec.train), digits = 3), y=20), colour="blue") + coord_cartesian(xlim = c(0, 1)) +  xlab("Recall Train Data")

p2 <- ggplot(df, aes(x=max.rec.test)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_vline(aes(xintercept=mean(max.rec.test)), color="red", linetype="dashed", size=1) +  geom_text(aes(x=mean(max.rec.test)+0.05, label=round(mean(max.rec.test), digits = 3), y=20), colour="red") +coord_cartesian(xlim = c(0, 1)) +
  xlab("Recall Test Data")

p3 <- ggplot(df, aes(x=max.pre.train)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_text(aes(x=mean(max.pre.train)+0.05, label=round(mean(max.pre.train), digits = 3), y=20), colour="blue") + geom_vline(aes(xintercept=mean(max.pre.train)), color="blue", linetype="dashed", size=1) + coord_cartesian(xlim = c(0, 1)) +  xlab("Precision Train Data")

p4 <- ggplot(df, aes(x=max.pre.test)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_text(aes(x=mean(max.pre.test)+0.05, label=round(mean(max.pre.test), digits = 3), y=20), colour="red") + geom_vline(aes(xintercept=mean(max.pre.test)), color="red", linetype="dashed", size=1) + coord_cartesian(xlim = c(0, 1)) +
  xlab("Precision Test Data")

grid.arrange(p1, p3,
             p2, p4, nrow=2)

## The result from testing dataset seems to deviate from that of the training dataset, indicating 
## that our model might be overfitting. Solution: collect more data to train our model could potentially
## reduce the overfitting problem. Due to data contraint in our case, however, we would use two
## techniques: PCR (dimension reduction) and Lasso Regression (coefficient shrinkage) and compare 
## their results.

```

#### 4. (Opetional) Restricted logistic regression

#### 1. Rationale: to test if by dropping some variables we can improve the model 

Make the restricted data set.
```{r}
data.train.restrict <- data.train[,-which(names(data.train) == "femaleres")] # training set
data.test.restrict <- data.test[,-which(names(data.test) == "femaleres")] # test set

```

Fit the logistic regression model and find: max F1 score and accuracy. Plot ROC curve.
```{r}
fit.restrict <- glm(depressed ~.,family=binomial(link='logit'),data=data.train.restrict)

# Your code to evaluate fit.restrict below
thetas <- seq(0.1, 0.6, by=0.01)
Y.test.restrict <- data.test.restrict[, "depressed"]
p.predict.logistic.restrict <- predict(fit.restrict,newdata=data.test.restrict,type='response')
out.restrict <- sweep.theta(thetas, p.predict.logistic.restrict, Y.test.restrict)
metrics.logistic.restrict <- out.restrict$evals.table

## visualize
# ggplot(metrics.logistic.restrict, aes(theta, accuracy, colour = "accuracy")) + 
#  geom_point(aes(theta, brier.score, colour = "brier")) +
#  geom_point(aes(theta, precision, colour = "precision")) +
#  geom_point(aes(theta, recall, colour = "recall")) +
#  geom_point(aes(theta, specificity, colour = "specificity")) +
#  geom_point(aes(theta, f1, colour = "f1")) +
#  geom_point() +
#  labs(colour = "Metrics") +
#  labs(y = "Ratio")

max.f1.test.restrict <- max(metrics.logistic.restrict$f1[is.finite(metrics.logistic.restrict$f1)]) # 0.345679
max.acc.test.restrict <- max(metrics.logistic.restrict$accuracy) #0.8253275

# Your code to plot ROC curve below

ggplot() +
  geom_line(data=metrics.logistic.restrict, aes((1-specificity), recall, colour = "restricted"))+
  geom_line(data=metrics.logistic, aes((1-specificity), recall, colour = "unrestricted")) +
  labs(colour = "Datasets")

# Comment: more space under the unrestricted line: better model

library(ROCR)
# unrestricted auc
pr <- prediction(p.predict.logistic, Y.test)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc # 0.7458333 

#restricted auc
pr.restricted <- prediction(p.predict.logistic.restrict, Y.test.restrict)
prf.restricted <- performance(pr.restricted, measure = "tpr", x.measure = "fpr")
auc.restricted <- performance(pr.restricted, measure = "auc")
auc.restricted <- auc.restricted@y.values[[1]]
auc.restricted # 0.75 better model


```

### LASSO logistic regression

#### 1. Rationale

To avoid overfitting we want to prevent our model from fitting to the noise in the
training data. Goal: smooth out the curve we are fitting.

#### 2. Fit LASSO logistic regression
```{r}
#fname.train <- "XXXX.csv"
#fname.test <- "XXXX.csv"
#data.train <- read.csv(fname.train, header = TRUE) # training set
#data.test <- read.csv(fname.test, header = TRUE)  # test set

# Get the data
Y.train <- data.train$depressed
X.train <- data.train[,-which(names(data.train) == "depressed")]

Y.test <- data.test$depressed
X.test <- data.test[,-which(names(data.test) == "depressed")]

# Set up data for lasso. Note that running lasso with cross validation might take a bit.
X.lasso <- model.matrix(~ . -1, X.train)
Y.lasso <- as.matrix(Y.train)
fit.lasso <- cv.glmnet(x=X.lasso, y=Y.lasso, nfolds = 10, family='binomial', type.measure = "class") # your code to fit lasso, hint: research glmnet's function cv.glmnet

plot(fit.lasso)

lambda_min <- fit.lasso$lambda.min # 0.01850701
#lambda_1se <- fit.lasso$lambda.1se # 0.05149686
coef(fit.lasso, s=lambda_min)

```

#### 3. Evaluate LASSO logistic regression
```{r}
thetas <- seq(0.1, 0.6, by=0.01)
X.lasso.test <- model.matrix(~ . -1, X.test)
p.predict.lasso <- predict(fit.lasso, newx = X.lasso.test, s=lambda_min, type='response')
out.lasso <- sweep.theta(thetas, p.predict.lasso, Y.test)
metrics.lasso <- out.lasso$evals.table

# Your code to plot ROC curve below

ggplot() +
  geom_line(data=metrics.logistic.restrict, aes((1-specificity), recall, colour = "restricted")) +
  geom_line(data=metrics.logistic, aes((1-specificity), recall, colour = "unrestricted")) +
  labs(colour = "Datasets") +
  geom_point(data=metrics.lasso, aes((1-specificity), recall, colour = "lasso"))

pr.lasso <- prediction(p.predict.lasso, Y.test)
prf.lasso <- performance(pr.lasso, measure = "tpr", x.measure = "fpr")
auc.lasso <- performance(pr.lasso, measure = "auc")
auc.lasso <- auc.lasso@y.values[[1]]
auc.lasso # Lasso regression auc 0.7239583 slightly worse than that of logistic regression 0.7458333, indicating that coefficient shrinkage does not properly address our overfitting problem.

```

