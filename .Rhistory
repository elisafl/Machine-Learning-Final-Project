K <- c(70:130)
ackley <- function (a, b, c, d, x) {
sum1 <- 0
sum2 <- 0
for (i in 1:d) {
sum1 <- sum1 + x[i]^2
}
for (j in 1:d) {
sum2 <- sum2 + cos(c * x[i])
}
f <- -a * exp(-b * sqrt(1 / d * sum1)) - exp(1 / d * sum2) + a + exp(1)
return (f)
}
a <- 20
b <- 0.2
c <- 2 * pi
d <- 2
x <- c(1,1)
ackley(a, b, c, d, x)
x <- runif(n = 1000)
y <- runif(n = 1000)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
inp <- cbind(x,y)
inp[, 3] <- NA
inp <- cbind(x, y, NA)
View(inp)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
x <- runif(n = 10000, min = -40, max = 40)
y <- runif(n = 10000, min = -40, max = 40)
inp <- cbind(x, y, NA)
y <- runif(n = 10000, min = -40, max = 40)
inp <- cbind(x, y, NA)
for (r in 1:nrow(inp)) {
inp[r, 3] <- ackley(a, b, c, d, inp[i, 1:2])
}
for (r in 1:nrow(inp)) {
inp[r, 3] <- ackley(a, b, c, d, inp[r, 1:2])
}
View(inp)
x <- runif(seq(c(-40,40)))
x <- runif(seq(c(-40,40)))
x <- seq(c(-40,40))
x <- seq(-40, 40, by = 0.01)
y = x
y <- x
out <- matrix(NA, nrow = length(x), nrow = length(y))
out <- matrix(NA, nrow = length(x), ncol = length(y))
x <- seq(-40, 40, by = 0.1)
y <- x
out <- matrix(NA, nrow = length(x), ncol = length(y))
x <- seq(-40, 40, by = 0.1)
y <- x
out <- matrix(NA, nrow = length(x), ncol = length(y))
for (r in 1:length(x)) {
for (c in 1:length(y)) {
inp <- c(x[r], y[c])
out[r, c] <- ackley(a, b, c, d, inp)
}
}
View(out)
library(plotly)
y
plot_ly(y = y, x = x, z = out) %>%
add_surface() %>%
layout(
title = "Minimum RSE function",
scene = list(
xaxis = list(title = "Batch-size"),
yaxis = list(title = "Epochs"),
zaxis = list(title = "RSE")))
x <- seq(-3, 3, by = 0.01)
y <- x
out <- matrix(NA, nrow = length(x), ncol = length(y))
for (r in 1:length(x)) {
for (c in 1:length(y)) {
inp <- c(x[r], y[c])
out[r, c] <- ackley(a, b, c, d, inp)
}
}
plot_ly(y = y, x = x, z = out) %>%
add_surface() %>%
layout(
title = "Minimum RSE function",
scene = list(
xaxis = list(title = "Batch-size"),
yaxis = list(title = "Epochs"),
zaxis = list(title = "RSE")))
load(MASS)
load("MASS")
attach("MASS")
attach(MASS)
library(MASS)
attach(Boston)
rm(list = ls())
```{r}
library(MASS)
attach(Boston)
data(Boston)
install.packages("tree")
library(MASS)
library(tree)
install.packages("tree")
install.packages("tree")
# Download data
```{r}
library(MASS)
library(tree)
attach(Boston)
data(Boston)
---
title: "Homework_10"
output: html_document
---
```{r}
rm(list = ls())
```
## Exercise 1
# Download data
```{r}
library(MASS)
library(tree)
attach(Boston)
data(Boston)
```
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
tree.boston = tree(medvâˆ¼., Boston)
tree.boston = tree(medv~.,Boston)
summary(tree_1)
tree_1 = tree(medv~.,Boston)
summary(tree_1)
plot(tree_1)
text(tree_1)
text(tree_1, pretty = 0)
plot(tree_1)
text(tree_1, pretty = 0)
tree_1 = tree(medv~.,Boston)
summary(tree_1)
plot(tree_1)
text(tree_1, pretty = 0)
install.packages("gmlnet")
library(glmnet)
rm(list = ls())
set.seed(132435)
data.full <- read.csv("train.csv")
library(data.table)
library(data.table)
library(data.table)
library(data.table)
install.packages("data.table")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
library(data.table)
install.packages("glmnet")
library(data.table)
# Load packages
```{r}
library(glmnet)
install.packages("glmnet")
library(glmnet)
library(data.table)
library(ggplot2)
install.packages("ggplot2")
install.packages("ROCR")
install.packages("gridExtra")
library(glmnet)
library(data.table)
library(ggplot2)
library(knitr)
library(ROCR)
library(gridExtra)
rm(list = ls())
set.seed(132435)
data.full <- read.csv("train.csv")
data.full <- read.csv("train.csv")
data.full <- data.full[sample(nrow(data.full)),-c(1,3)] # drop irrelevant variables for analysis, randomize data
missing_prop <- sapply(data.full,
function(x){mean(is.na(x))})
data.full <- data.full[missing_prop <= 0] # drop varaiables with missing data
data.train <- data.full[1:floor(nrow(data.full)*0.8),] # training set
data.test <- data.full[ceiling(nrow(data.full)*0.8):nrow(data.full), ] # test set
# Your code to summarize data.train below.
summary(data.train)
fit <- glm(depressed ~., family=binomial(link='logit'), data=data.train, ) # fit your logistic model to the training set here.
compute.metrics = function(y.predict, p.predict, y.test) {
# Parameters
# y.predict : predictions generated by some model.
# p.predict : predicted probabilities generated by some model.
# y.test : outcomes in test data set.
N.correct <- sum(y.predict==y.test,na.rm=TRUE)
accuracy <- N.correct/length(y.test)
brier.score <- (1/length(p.predict)) * sum((y.test-p.predict)^2)
true.positive <- sum(y.predict == y.test & y.predict > 0, na.rm=TRUE)
false.positive <- sum(y.predict != y.test & y.predict > 0, na.rm=TRUE)
true.negative <- sum(y.predict == y.test & y.predict == 0, na.rm=TRUE)
false.negative <- sum(y.predict != y.test & y.predict == 0, na.rm=TRUE)
precision <- true.positive/(true.positive + false.positive)
recall <- true.positive/(true.positive+false.negative)
specificity <- true.negative/(false.positive+true.negative)
f1 <- 2*(1/recall + 1/precision)^(-1) # F1 score
return(data.table(accuracy=round(accuracy, digits = 3), brier.score=round(brier.score, digits = 3),
true.positive=true.positive,
false.positive=false.positive,
true.negative=true.negative,
false.negative=false.negative,
precision= round(precision, digits = 3), recall= round(recall, digits = 3),
specificity= round(specificity, digits = 3), f1= round(f1, digits = 3)))
}
evaluate.model <- function(p.predict, theta, y.test) {
# Parameters
# p.predict : predicted probabilities generated by some model.
# theta: decision threshold
# y.test : outcomes in test data set
y.predict <- ifelse(p.predict>=theta, 1, 0) # predictions given decision threshold theta
out <- compute.metrics(y.predict, p.predict, y.test)
return(out)
}
sweep.theta = function(thetas, p.predict, y.test) {
# Parameters
# thetas : range of thetas to sweep
# p.predict : predicted probabilities generated by some model.
# y.test : outcomes in test data set
evals = list()
for (i in 1:length(thetas)) {
c.eval <- evaluate.model(p.predict,thetas[i],y.test) # evaluate model at theta[i]
evals <- c(evals, list(c.eval))
}
evals.table = rbindlist(evals)
evals.table$theta = thetas
evals.table.m = melt(evals.table[, -c('true.positive',
'true.negative',
'false.positive',
'false.negative'), with=F], id.var='theta')
return(list(evals.table = evals.table,
evals.table.m=evals.table.m))
}
fit <- glm(depressed ~., family=binomial(link='logit'), data=data.train) # fit your logistic model to the training set here.
Y.test <- data.test[, c("depressed")]
X.test <- data.test[,-which(names(data.test) == "depressed")]
thetas <- seq(0,0.6, by=0.01)
p.predict.logistic <- predict(fit, newdata=data.test, type='response') # predicted probabilities generated by fit (logistic regression model)
out <- sweep.theta(thetas, p.predict.logistic, Y.test)
to.plot.logistic <- out$evals.table.m
metrics.logistic <- out$evals.table
# Your plot here. Hint: use ggplot.
ggplot(metrics.logistic, aes(theta, accuracy, colour = "accuracy")) +
geom_line() +
geom_line(aes(theta, brier.score, colour = "brier")) +
geom_line(aes(theta, precision, colour = "precision")) +
geom_line(aes(theta, recall, colour = "recall")) +
geom_line(aes(theta, specificity, colour = "specificity")) +
geom_line(aes(theta, f1, colour = "f1")) +
labs(colour = "Metrics") +
labs(y = "Ratio")
max.acc.theta <- 0.15
Y.train <- data.train[, "depressed"]
p.predict.logistic.train <- predict(fit,newdata=data.train,type='response') # in-sample predicted probabilities generated by fit (logistic regression model)
confusion.matrix.train <- evaluate.model(p.predict.logistic.train, max.acc.theta, Y.train)
confusion.matrix.test <- evaluate.model(p.predict.logistic, max.acc.theta, Y.test)
confusion.matrix <- rbind(confusion.matrix.train, confusion.matrix.test)
rownames(confusion.matrix) <- c("train","test")
confusion.matrix
## k-fold cross validation
## goal: determine if we are overfitting our model with logistic regression
k <- 100
max.acc.theta <- 0.15 ## selected to achieve the highest recall while getting accaptable results from other metrices
max.rec.test <- NULL
max.pre.test <- NULL
max.rec.train <- NULL
max.pre.train <- NULL
for(i in 1:k){
data.full <- read.csv("train.csv")
data.full <- data.full[sample(nrow(data.full)),-c(1,3)] # drop irrelevant variables for analysis, randomize data
missing_prop <- sapply(data.full,
function(x){mean(is.na(x))})
data.full <- data.full[missing_prop <= 0] # drop varaiables with missing data
data.train <- data.full[1:floor(nrow(data.full)*0.95),] # training set
data.test <- data.full[ceiling(nrow(data.full)*0.95):nrow(data.full), ] # test set
fit <- glm(depressed ~., family=binomial(link='logit'), data=data.train, ) # fit your logistic model to the training set here.
Y.test <- data.test[, c("depressed")]
p.predict.logistic <- predict(fit, newdata=data.test, type='response') # out-sample predicted probabilities generated by fit (logistic regression model)
max.rec.test[i] <- evaluate.model(p.predict.logistic, max.acc.theta, Y.test)$recall
max.pre.test[i] <- evaluate.model(p.predict.logistic, max.acc.theta, Y.test)$precision
Y.train <- data.train[, "depressed"]
p.predict.logistic.train <- predict(fit,newdata=data.train,type='response') # in-sample predicted probabilities generated by fit (logistic regression model)
max.rec.train[i] <- evaluate.model(p.predict.logistic.train, max.acc.theta, Y.train)$recall
max.pre.train[i] <- evaluate.model(p.predict.logistic.train, max.acc.theta, Y.train)$precision
}
output <- matrix(data=NA, ncol=2, nrow =2)
rownames(output) <- c("train","test")
colnames(output) <- c("Recall", "Precision")
output[1,] <- c(mean(max.rec.train), mean(max.pre.train))
output[2,] <- c(mean(max.rec.test), mean(max.pre.test))
output
df <- data.frame(t(rbind(max.rec.train, max.rec.test, max.pre.train, max.pre.test)))
# histogram
p1 <- ggplot(df, aes(x=max.rec.train)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_vline(aes(xintercept=mean(max.rec.train)), color="blue", linetype="dashed", size=1) + geom_text(aes(x=mean(max.rec.train)+0.05, label=round(mean(max.rec.train), digits = 3), y=20), colour="blue") + coord_cartesian(xlim = c(0, 1)) +  xlab("Recall Train Data")
p2 <- ggplot(df, aes(x=max.rec.test)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_vline(aes(xintercept=mean(max.rec.test)), color="red", linetype="dashed", size=1) +  geom_text(aes(x=mean(max.rec.test)+0.05, label=round(mean(max.rec.test), digits = 3), y=20), colour="red") +coord_cartesian(xlim = c(0, 1)) +
xlab("Recall Test Data")
p3 <- ggplot(df, aes(x=max.pre.train)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_text(aes(x=mean(max.pre.train)+0.05, label=round(mean(max.pre.train), digits = 3), y=20), colour="blue") + geom_vline(aes(xintercept=mean(max.pre.train)), color="blue", linetype="dashed", size=1) + coord_cartesian(xlim = c(0, 1)) +  xlab("Precision Train Data")
p4 <- ggplot(df, aes(x=max.pre.test)) + geom_histogram(color="black", fill="white", binwidth = 0.01) + geom_text(aes(x=mean(max.pre.test)+0.05, label=round(mean(max.pre.test), digits = 3), y=20), colour="red") + geom_vline(aes(xintercept=mean(max.pre.test)), color="red", linetype="dashed", size=1) + coord_cartesian(xlim = c(0, 1)) +
xlab("Precision Test Data")
grid.arrange(p1, p3,
p2, p4, nrow=2)
## The result from testing dataset seems to deviate from that of the training dataset, indicating
## that our model might be overfitting. Solution: collect more data to train our model could potentially
## reduce the overfitting problem. Due to data contraint in our case, however, we would use two
## techniques: PCR (dimension reduction) and Lasso Regression (coefficient shrinkage) and compare
## their results.
data.train.restrict <- data.train[,-which(names(data.train) == "femaleres")] # training set
data.test.restrict <- data.test[,-which(names(data.test) == "femaleres")] # test set
fit.restrict <- glm(depressed ~.,family=binomial(link='logit'),data=data.train.restrict)
# Your code to evaluate fit.restrict below
thetas <- seq(0.1, 0.6, by=0.01)
Y.test.restrict <- data.test.restrict[, "depressed"]
p.predict.logistic.restrict <- predict(fit.restrict,newdata=data.test.restrict,type='response')
out.restrict <- sweep.theta(thetas, p.predict.logistic.restrict, Y.test.restrict)
metrics.logistic.restrict <- out.restrict$evals.table
## visualize
# ggplot(metrics.logistic.restrict, aes(theta, accuracy, colour = "accuracy")) +
#  geom_point(aes(theta, brier.score, colour = "brier")) +
#  geom_point(aes(theta, precision, colour = "precision")) +
#  geom_point(aes(theta, recall, colour = "recall")) +
#  geom_point(aes(theta, specificity, colour = "specificity")) +
#  geom_point(aes(theta, f1, colour = "f1")) +
#  geom_point() +
#  labs(colour = "Metrics") +
#  labs(y = "Ratio")
max.f1.test.restrict <- max(metrics.logistic.restrict$f1[is.finite(metrics.logistic.restrict$f1)]) # 0.345679
max.acc.test.restrict <- max(metrics.logistic.restrict$accuracy) #0.8253275
# Your code to plot ROC curve below
ggplot() +
geom_line(data=metrics.logistic.restrict, aes((1-specificity), recall, colour = "restricted"))+
geom_line(data=metrics.logistic, aes((1-specificity), recall, colour = "unrestricted")) +
labs(colour = "Datasets")
# Comment: more space under the unrestricted line: better model
library(ROCR)
# unrestricted auc
pr <- prediction(p.predict.logistic, Y.test)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc # 0.7458333
#restricted auc
pr.restricted <- prediction(p.predict.logistic.restrict, Y.test.restrict)
prf.restricted <- performance(pr.restricted, measure = "tpr", x.measure = "fpr")
auc.restricted <- performance(pr.restricted, measure = "auc")
auc.restricted <- auc.restricted@y.values[[1]]
auc.restricted # 0.75 better model
#fname.train <- "XXXX.csv"
#fname.test <- "XXXX.csv"
#data.train <- read.csv(fname.train, header = TRUE) # training set
#data.test <- read.csv(fname.test, header = TRUE)  # test set
# Get the data
Y.train <- data.train$depressed
X.train <- data.train[,-which(names(data.train) == "depressed")]
Y.test <- data.test$depressed
X.test <- data.test[,-which(names(data.test) == "depressed")]
# Set up data for lasso. Note that running lasso with cross validation might take a bit.
X.lasso <- model.matrix(~ . -1, X.train)
Y.lasso <- as.matrix(Y.train)
fit.lasso <- cv.glmnet(x=X.lasso, y=Y.lasso, nfolds = 10, family='binomial', type.measure = "class") # your code to fit lasso, hint: research glmnet's function cv.glmnet
plot(fit.lasso)
lambda_min <- fit.lasso$lambda.min # 0.01850701
#lambda_1se <- fit.lasso$lambda.1se # 0.05149686
coef(fit.lasso, s=lambda_min)
thetas <- seq(0.1, 0.6, by=0.01)
X.lasso.test <- model.matrix(~ . -1, X.test)
p.predict.lasso <- predict(fit.lasso, newx = X.lasso.test, s=lambda_min, type='response')
out.lasso <- sweep.theta(thetas, p.predict.lasso, Y.test)
metrics.lasso <- out.lasso$evals.table
# Your code to plot ROC curve below
ggplot() +
geom_line(data=metrics.logistic.restrict, aes((1-specificity), recall, colour = "restricted")) +
geom_line(data=metrics.logistic, aes((1-specificity), recall, colour = "unrestricted")) +
labs(colour = "Datasets") +
geom_point(data=metrics.lasso, aes((1-specificity), recall, colour = "lasso"))
pr.lasso <- prediction(p.predict.lasso, Y.test)
prf.lasso <- performance(pr.lasso, measure = "tpr", x.measure = "fpr")
auc.lasso <- performance(pr.lasso, measure = "auc")
auc.lasso <- auc.lasso@y.values[[1]]
auc.lasso # Lasso regression auc 0.7239583 slightly worse than that of logistic regression 0.7458333, indicating that coefficient shrinkage does not properly address our overfitting problem.
rm(list = ls())
# Load packages
```{r}
library(glmnet)
library(glmnet)
library(data.table)
library(ggplot2)
library(knitr)
library(ROCR)
library(gridExtra)
library(rstudioapi)
# Prepare
```{r}
rm(list = ls())
set.seed(132435)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```{r}
data.full <- read.csv("train.csv")
data.full <- data.full[sample(nrow(data.full)),-c(1,3)] # drop irrelevant variables for analysis, randomize data
missing_prop <- sapply(data.full,
function(x){mean(is.na(x))})
data.full <- data.full[missing_prop <= 0] # drop varaiables with missing data
data.train <- data.full[1:floor(nrow(data.full)*0.8),] # training set
data.test <- data.full[ceiling(nrow(data.full)*0.8):nrow(data.full), ] # test set
# Your code to summarize data.train below.
summary(data.train)
lm(depressed ~., data = data.train)
linear_model <- lm(depressed ~., data = data.train)
summary(linear_model)
plot(data.train, type = "o")
plot(data.train, type = "l")
plot(data.train)
tableplot(data.train)
# Load packages
library(glmnet)
library(data.table)
library(ggplot2)
library(knitr)
library(ROCR)
library(gridExtra)
library(rstudioapi)
# Prepare
rm(list = ls())
set.seed(132435)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data.full <- read.csv("train.csv")
data.full <- data.full[sample(nrow(data.full)), -c(1, 3)] # drop irrelevant variables for analysis, randomize data
missing_prop <- sapply(data.full, function(x) {mean(is.na(x))})
data.full <- data.full[missing_prop <= 0] # drop varaiables with missing data
data.train <- data.full[1:floor(nrow(data.full) * 0.8), ] # training set
data.test <- data.full[ceiling(nrow(data.full) * 0.8):nrow(data.full), ] # test set
# Your code to summarize data.train below.
linear_model <- lm(depressed ~., data = data.train)
summary(linear_model)
tableplot(data.train)
require(ggplot2)
tableplot(data.train)
install.packages("tabplot")
# Descriptive statistics
library(tabplot)
linear_model <- lm(depressed ~., data = data.train)
summary(linear_model)
tableplot(data.train)
# Descriptive statistics
library(broom)
install.packages("broom")
# Descriptive statistics
library(broom)
tidy(linear_model)
library(knitr)
kable(tidy(linear_model))
output <- data.frame()
output$Variable <- colnames(data.train)
colnames(data.train)
output <- data.frame(matrix(NA, nrow = ncol(data_train) - 1, ncol = 5))
output <- data.frame(matrix(NA, nrow = ncol(data.train) - 1, ncol = 5))
View(output)
output$Variable <- colnames(data.train)
output$Variable <- colnames(data.train)[-39]
View(output)
colnames(output) <- c("Variable", "Estimate", "Std. Error", "Statistic", "P-value")
output$Variable <- colnames(data.train)[-39]
output <- data.frame(matrix(NA, nrow = ncol(data.train) - 1, ncol = 5))
colnames(output) <- c("Variable", "Estimate", "Std. Error", "Statistic", "P-value")
output$Variable <- colnames(data.train)[-39]
output$Estimate <- linear_model$coefficients
linear_model
output$Variable <- c("Intercept", colnames(data.train)[-39])
output <- data.frame(matrix(NA, nrow = ncol(data.train), ncol = 5))
colnames(output) <- c("Variable", "Estimate", "Std. Error", "Statistic", "P-value")
output$Variable <- c("Intercept", colnames(data.train)[-39])
output$Estimate <- linear_model$coefficients
summary(linear_model)
output <- data.frame(matrix(NA, nrow = ncol(data.train), ncol = 5))
colnames(output) <- c("Variable", "Estimate", "Std. Error", "Statistic", "P-value")
output$Variable <- c("Intercept", colnames(data.train)[-39])
output$Estimate <- linear_model$coefficients
output$`Std. Error` <- linear_model$residuals
output$Statistic <- linear_model$df.residual
output$`P-value` <- linear_model$call
sum_linear_model <- summary(linear_model)
output$Estimate <- sum_linear_model$terms
linear_model <- lm(depressed ~., data = data.train)
sum_linear_model <- summary(linear_model)
sum_linear_model
output$Estimate <- sum_linear_model$coefficients
sum_linear_model$coefficients
data.frame(sum_linear_model$coefficients)
d <- data.frame(sum_linear_model$coefficients)
colnames(d) <- c("Estimate", "Std. Error", "t-Value", "p-Value")
d$sign <- ifelse(d$`p-Value` < 0.01, "***", ifelse(d$`p-Value` < 0.05, "**", ifelse(d$`p-Value` < 0.1, "*","")))
d
d$Significance <- ifelse(d$`p-Value` < 0.01, "***", ifelse(d$`p-Value` < 0.05, "**", ifelse(d$`p-Value` < 0.1, "*","")))
write.csv(d, "/output/linear_model.csv")
write.csv(d, "/output/linear_model.csv")
write.csv(d, "output/linear_model.csv")
